{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "import os, json\n",
    "from scipy import misc\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_builder import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DATA_PATH = './data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = pd.read_csv(os.path.join(IMAGE_DATA_PATH, 'driving_log.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8036 entries, 0 to 8035\n",
      "Data columns (total 7 columns):\n",
      "center      8036 non-null object\n",
      "left        8036 non-null object\n",
      "right       8036 non-null object\n",
      "steering    8036 non-null float64\n",
      "throttle    8036 non-null float64\n",
      "brake       8036 non-null float64\n",
      "speed       8036 non-null float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 439.5+ KB\n"
     ]
    }
   ],
   "source": [
    "image_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>steering</th>\n",
       "      <th>throttle</th>\n",
       "      <th>brake</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMG/center_2016_12_01_13_30_48_287.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_30_48_287.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_30_48_287.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.148290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMG/center_2016_12_01_13_30_48_404.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_30_48_404.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_30_48_404.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.879630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMG/center_2016_12_01_13_31_12_937.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_31_12_937.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_31_12_937.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.453011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IMG/center_2016_12_01_13_31_13_037.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_31_13_037.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_31_13_037.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.438419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IMG/center_2016_12_01_13_31_13_177.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_31_13_177.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_31_13_177.jpg</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.418236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   center  \\\n",
       "0  IMG/center_2016_12_01_13_30_48_287.jpg   \n",
       "1  IMG/center_2016_12_01_13_30_48_404.jpg   \n",
       "2  IMG/center_2016_12_01_13_31_12_937.jpg   \n",
       "3  IMG/center_2016_12_01_13_31_13_037.jpg   \n",
       "4  IMG/center_2016_12_01_13_31_13_177.jpg   \n",
       "\n",
       "                                    left  \\\n",
       "0   IMG/left_2016_12_01_13_30_48_287.jpg   \n",
       "1   IMG/left_2016_12_01_13_30_48_404.jpg   \n",
       "2   IMG/left_2016_12_01_13_31_12_937.jpg   \n",
       "3   IMG/left_2016_12_01_13_31_13_037.jpg   \n",
       "4   IMG/left_2016_12_01_13_31_13_177.jpg   \n",
       "\n",
       "                                    right  steering  throttle  brake  \\\n",
       "0   IMG/right_2016_12_01_13_30_48_287.jpg       0.0       0.0    0.0   \n",
       "1   IMG/right_2016_12_01_13_30_48_404.jpg       0.0       0.0    0.0   \n",
       "2   IMG/right_2016_12_01_13_31_12_937.jpg       0.0       0.0    0.0   \n",
       "3   IMG/right_2016_12_01_13_31_13_037.jpg       0.0       0.0    0.0   \n",
       "4   IMG/right_2016_12_01_13_31_13_177.jpg       0.0       0.0    0.0   \n",
       "\n",
       "       speed  \n",
       "0  22.148290  \n",
       "1  21.879630  \n",
       "2   1.453011  \n",
       "3   1.438419  \n",
       "4   1.418236  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "ind_train, ind_test = train_test_split(np.array(range(len(image_data))), train_size=0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = image_data.loc[ind_train]\n",
    "test_df = image_data.loc[ind_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_df(samples_df_, source_path='data', data_columns = ['center'], val_column = 'steering', batch_size=4):\n",
    "# yields batches from dataframe samples_df: ['images', 'steering']\n",
    "    samples_df = samples_df_.copy()\n",
    "    num_samples = len(samples_df)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples_df)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples_df[offset:offset+batch_size]\n",
    "\n",
    "            images = None\n",
    "            angles = np.array([], dtype='float32')\n",
    "            for i, batch_sample in batch_samples.iterrows():\n",
    "                name = batch_sample[np.random.choice(data_columns, 1)].values[0]\n",
    "                name = name.strip()\n",
    "                center_image = cv2.imread(os.path.join(source_path,name))\n",
    "                if center_image is not None:\n",
    "                    center_angle = batch_sample[val_column]\n",
    "                    if images is None:\n",
    "                        images = center_image[np.newaxis]\n",
    "                    else:\n",
    "                        images = np.vstack([images, center_image[np.newaxis]])\n",
    "                    angles = np.append(angles, center_angle)\n",
    "\n",
    "            yield images, angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_generator_df(samples_df_, source_path='data', data_columns = ['center'], batch_size=4):\n",
    "# yields batches for predictions, no shuffling\n",
    "    samples_df = samples_df_.copy()\n",
    "    num_samples = len(samples_df)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples_df[offset:offset+batch_size]\n",
    "\n",
    "            images = None\n",
    "            angles = np.array([], dtype='float32')\n",
    "            for i, batch_sample in batch_samples.iterrows():\n",
    "                name = batch_sample[np.random.choice(data_columns, 1)].values[0]\n",
    "                name = name.strip()\n",
    "                center_image = cv2.imread(os.path.join(source_path,name))\n",
    "                center_angle = batch_sample['steering']\n",
    "                if images is None:\n",
    "                    images = center_image[np.newaxis]\n",
    "                else:\n",
    "                    images = np.vstack([images, center_image[np.newaxis]])\n",
    "                angles = np.append(angles, center_angle)\n",
    "\n",
    "            yield images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_generator = generator_df(train_df, batch_size=BATCH_SIZE)\n",
    "val_generator = generator_df(test_df, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training VGG16-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVING_PATH = 'trained_weights'\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'vgg16', 'weights_vgg16.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = get_model('VGG16')\n",
    "model_vgg16.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "804/803 [==============================] - 23s 29ms/step - loss: 0.2219 - val_loss: 0.0153\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/vgg16/weights_vgg16.01-0.0153.hdf5\n",
      "Epoch 2/10\n",
      "804/803 [==============================] - 19s 24ms/step - loss: 0.0140 - val_loss: 0.0200\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/vgg16/weights_vgg16.02-0.0200.hdf5\n",
      "Epoch 3/10\n",
      "804/803 [==============================] - 19s 24ms/step - loss: 0.0121 - val_loss: 0.0109\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/vgg16/weights_vgg16.03-0.0109.hdf5\n",
      "Epoch 4/10\n",
      "804/803 [==============================] - 19s 24ms/step - loss: 0.0099 - val_loss: 0.0231\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/vgg16/weights_vgg16.04-0.0231.hdf5\n",
      "Epoch 5/10\n",
      "804/803 [==============================] - 19s 24ms/step - loss: 0.0087 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/vgg16/weights_vgg16.05-0.0095.hdf5\n",
      "Epoch 6/10\n",
      "804/803 [==============================] - 19s 24ms/step - loss: 0.0078 - val_loss: 0.0100\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/vgg16/weights_vgg16.06-0.0100.hdf5\n",
      "Epoch 7/10\n",
      "804/803 [==============================] - 19s 24ms/step - loss: 0.0074 - val_loss: 0.0117\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/vgg16/weights_vgg16.07-0.0117.hdf5\n",
      "Epoch 8/10\n",
      "804/803 [==============================] - 19s 24ms/step - loss: 0.0072 - val_loss: 0.0123\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/vgg16/weights_vgg16.08-0.0123.hdf5\n",
      "Epoch 9/10\n",
      "804/803 [==============================] - 19s 24ms/step - loss: 0.0073 - val_loss: 0.0112\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/vgg16/weights_vgg16.09-0.0112.hdf5\n",
      "Epoch 10/10\n",
      "804/803 [==============================] - 19s 24ms/step - loss: 0.0071 - val_loss: 0.0103\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/vgg16/weights_vgg16.10-0.0103.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_vgg16 = model_vgg16.fit_generator(train_generator, steps_per_epoch=len(train_df)/BATCH_SIZE, validation_data=val_generator,\n",
    "                                       validation_steps=len(test_df)/BATCH_SIZE, epochs=10, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training NVIDIA-style model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'nvidia', 'weights_nvidia.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nvidia = get_model('NVIDIA')\n",
    "model_nvidia.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "804/803 [==============================] - 14s 18ms/step - loss: 0.6236 - val_loss: 0.0154\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/nvidia/weights_nvidia.01-0.0154.hdf5\n",
      "Epoch 2/10\n",
      "804/803 [==============================] - 14s 17ms/step - loss: 0.0098 - val_loss: 0.0135\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/nvidia/weights_nvidia.02-0.0135.hdf5\n",
      "Epoch 3/10\n",
      "804/803 [==============================] - 14s 17ms/step - loss: 0.0052 - val_loss: 0.0154\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/nvidia/weights_nvidia.03-0.0154.hdf5\n",
      "Epoch 4/10\n",
      "804/803 [==============================] - 14s 17ms/step - loss: 0.0032 - val_loss: 0.0142\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/nvidia/weights_nvidia.04-0.0142.hdf5\n",
      "Epoch 5/10\n",
      "804/803 [==============================] - 14s 17ms/step - loss: 0.0036 - val_loss: 0.0142\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/nvidia/weights_nvidia.05-0.0142.hdf5\n",
      "Epoch 6/10\n",
      "804/803 [==============================] - 14s 17ms/step - loss: 0.0038 - val_loss: 0.0144\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/nvidia/weights_nvidia.06-0.0144.hdf5\n",
      "Epoch 7/10\n",
      "804/803 [==============================] - 14s 17ms/step - loss: 0.0049 - val_loss: 0.0138\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/nvidia/weights_nvidia.07-0.0138.hdf5\n",
      "Epoch 8/10\n",
      "804/803 [==============================] - 14s 17ms/step - loss: 0.0078 - val_loss: 0.0140\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/nvidia/weights_nvidia.08-0.0140.hdf5\n",
      "Epoch 9/10\n",
      "804/803 [==============================] - 14s 17ms/step - loss: 1932740.7785 - val_loss: 262.9875\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/nvidia/weights_nvidia.09-262.9875.hdf5\n",
      "Epoch 10/10\n",
      "804/803 [==============================] - 14s 17ms/step - loss: 195.4979 - val_loss: 96.1313\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/nvidia/weights_nvidia.10-96.1313.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_nvidia = model_nvidia.fit_generator(train_generator, steps_per_epoch=len(train_df)/BATCH_SIZE, validation_data=val_generator,\n",
    "                                       validation_steps=len(test_df)/BATCH_SIZE, epochs=10, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_vgg16.history['val_loss'])\n",
    "plt.plot(history_nvidia.history['val_loss'])\n",
    "plt.ylim((0,0.025))\n",
    "plt.legend(['vgg16', 'nvidia'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained model (for experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16.load_weights('trained_weights/vgg16/weights_vgg16.05-0.0095.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to use left and right images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_generator = predict_generator_df(train_df, source_path='data', data_columns = ['center'], batch_size=BATCH_SIZE)\n",
    "left_generator = predict_generator_df(train_df, data_columns=['left'], batch_size=BATCH_SIZE)\n",
    "right_generator = predict_generator_df(train_df, data_columns=['right'], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_predictions = model_vgg16.predict_generator(center_generator, steps=len(train_df)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_predictions = center_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_predictions = model_vgg16.predict_generator(left_generator, steps=len(train_df)/BATCH_SIZE).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_predictions = model_vgg16.predict_generator(right_generator, steps=len(train_df)/BATCH_SIZE).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we calculate correction factors between predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_delta = np.mean(center_predictions - left_predictions)\n",
    "right_delta = np.mean(center_predictions - right_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05206264 0.059707746\n"
     ]
    }
   ],
   "source": [
    "print(left_delta, right_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make extended dataset with all images used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_steering = train_df['steering'].values - left_delta\n",
    "right_steering = train_df['steering'].values - right_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>steering</th>\n",
       "      <th>throttle</th>\n",
       "      <th>brake</th>\n",
       "      <th>speed</th>\n",
       "      <th>left_steering</th>\n",
       "      <th>right_steering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>IMG/center_2016_12_01_13_38_25_085.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_38_25_085.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_38_25_085.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.186300</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>-0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6214</th>\n",
       "      <td>IMG/center_2016_12_01_13_43_37_055.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_43_37_055.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_43_37_055.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.186570</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>-0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>IMG/center_2016_12_01_13_38_33_072.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_38_33_072.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_38_33_072.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.823975</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>-0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>IMG/center_2016_12_01_13_37_10_714.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_37_10_714.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_37_10_714.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.186650</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>-0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>IMG/center_2016_12_01_13_41_36_264.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_41_36_264.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_41_36_264.jpg</td>\n",
       "      <td>-0.135712</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.185320</td>\n",
       "      <td>-0.083649</td>\n",
       "      <td>-0.195420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      center  \\\n",
       "3422  IMG/center_2016_12_01_13_38_25_085.jpg   \n",
       "6214  IMG/center_2016_12_01_13_43_37_055.jpg   \n",
       "3501  IMG/center_2016_12_01_13_38_33_072.jpg   \n",
       "2688  IMG/center_2016_12_01_13_37_10_714.jpg   \n",
       "5023  IMG/center_2016_12_01_13_41_36_264.jpg   \n",
       "\n",
       "                                       left  \\\n",
       "3422   IMG/left_2016_12_01_13_38_25_085.jpg   \n",
       "6214   IMG/left_2016_12_01_13_43_37_055.jpg   \n",
       "3501   IMG/left_2016_12_01_13_38_33_072.jpg   \n",
       "2688   IMG/left_2016_12_01_13_37_10_714.jpg   \n",
       "5023   IMG/left_2016_12_01_13_41_36_264.jpg   \n",
       "\n",
       "                                       right  steering  throttle  brake  \\\n",
       "3422   IMG/right_2016_12_01_13_38_25_085.jpg  0.000000  0.985533    0.0   \n",
       "6214   IMG/right_2016_12_01_13_43_37_055.jpg  0.000000  0.985533    0.0   \n",
       "3501   IMG/right_2016_12_01_13_38_33_072.jpg  0.000000  0.000000    0.0   \n",
       "2688   IMG/right_2016_12_01_13_37_10_714.jpg  0.000000  0.985533    0.0   \n",
       "5023   IMG/right_2016_12_01_13_41_36_264.jpg -0.135712  0.985533    0.0   \n",
       "\n",
       "          speed  left_steering  right_steering  \n",
       "3422  30.186300       0.052063       -0.059708  \n",
       "6214  30.186570       0.052063       -0.059708  \n",
       "3501   9.823975       0.052063       -0.059708  \n",
       "2688  30.186650       0.052063       -0.059708  \n",
       "5023  30.185320      -0.083649       -0.195420  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ext = train_df\n",
    "train_df_ext['left_steering'] = train_df['steering'] - left_delta\n",
    "train_df_ext['right_steering'] = train_df['steering'] - right_delta\n",
    "train_df_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19284 entries, 0 to 19283\n",
      "Data columns (total 2 columns):\n",
      "image    19284 non-null object\n",
      "value    19284 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 301.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df_joint = train_df_ext[['center', 'steering']].rename(columns={'center': 'image', 'steering': 'value'})\n",
    "train_df_joint = pd.concat([train_df_joint,\n",
    "                           train_df_ext[['left', 'left_steering']].rename(columns={'left': 'image', 'left_steering': 'value'})],\n",
    "                          ignore_index=True)\n",
    "train_df_joint = pd.concat([train_df_joint,\n",
    "                           train_df_ext[['right', 'right_steering']].rename(columns={'right': 'image', 'right_steering': 'value'})],\n",
    "                          ignore_index=True)\n",
    "train_df_joint.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VGG16 model on joint dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_joint = generator_df(train_df_joint, data_columns = ['image'], val_column = 'value', batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'vgg16_joint', 'weights_vgg16_joint.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16_joint = get_model('VGG16')\n",
    "model_vgg16_joint.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0665 - val_loss: 0.0099\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.01-0.0099.hdf5\n",
      "Epoch 2/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.02-0.0095.hdf5\n",
      "Epoch 3/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0085 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.03-0.0089.hdf5\n",
      "Epoch 4/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0085 - val_loss: 0.0093\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.04-0.0093.hdf5\n",
      "Epoch 5/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0082 - val_loss: 0.0090\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.05-0.0090.hdf5\n",
      "Epoch 6/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0078 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.06-0.0089.hdf5\n",
      "Epoch 7/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0075 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.07-0.0089.hdf5\n",
      "Epoch 8/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0072 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.08-0.0095.hdf5\n",
      "Epoch 9/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0071 - val_loss: 0.0094\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.09-0.0094.hdf5\n",
      "Epoch 10/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.10-0.0095.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_vgg16_joint = model_vgg16_joint.fit_generator(train_generator_joint, steps_per_epoch=len(train_df_joint)/BATCH_SIZE,\n",
    "                                                        validation_data=val_generator,\n",
    "                                       validation_steps=len(test_df)/BATCH_SIZE, epochs=10, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NVIDIA-model on joint dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'nvidia_joint', 'weights_nvidia_joint.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nvidia_joint = get_model('NVIDIA')\n",
    "model_nvidia_joint.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2411/2410 [==============================] - 37s 15ms/step - loss: 0.3003 - val_loss: 0.0110\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.01-0.0110.hdf5\n",
      "Epoch 2/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0094 - val_loss: 0.0108\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.02-0.0108.hdf5\n",
      "Epoch 3/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0090 - val_loss: 0.0145\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.03-0.0145.hdf5\n",
      "Epoch 4/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0086 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.04-0.0194.hdf5\n",
      "Epoch 5/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0102 - val_loss: 0.0123\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.05-0.0123.hdf5\n",
      "Epoch 6/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0102 - val_loss: 0.0136\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.06-0.0136.hdf5\n",
      "Epoch 7/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 1148.6888 - val_loss: 0.5850\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.07-0.5850.hdf5\n",
      "Epoch 8/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.2296 - val_loss: 0.0756\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.08-0.0756.hdf5\n",
      "Epoch 9/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0617 - val_loss: 0.0469\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.09-0.0469.hdf5\n",
      "Epoch 10/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0639 - val_loss: 0.0447\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.10-0.0447.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_nvidia_joint = model_nvidia_joint.fit_generator(train_generator_joint, steps_per_epoch=len(train_df_joint)/BATCH_SIZE,\n",
    "                                                        validation_data=val_generator,\n",
    "                                       validation_steps=len(test_df)/BATCH_SIZE, epochs=10, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_img_path = os.path.join( 'IMG', 'flipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "values = []\n",
    "for i, row in train_df_joint.iterrows():\n",
    "    imgname = row['image']\n",
    "    steering = row['value']\n",
    "    imgname = imgname.strip()\n",
    "    #img = cv2.imread(os.path.join('data',imgname))\n",
    "    #img = cv2.flip(img, flipCode=1)\n",
    "    saving_name = os.path.join(flipped_img_path, os.path.split(imgname)[-1])\n",
    "    #cv2.imwrite(saving_name, img)\n",
    "    filenames.append(saving_name)\n",
    "    values.append(-1*steering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_df = pd.DataFrame({'image': filenames, 'value': values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38568 entries, 0 to 38567\n",
      "Data columns (total 2 columns):\n",
      "image    38568 non-null object\n",
      "value    38568 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 602.7+ KB\n"
     ]
    }
   ],
   "source": [
    "flipped_df = pd.concat([train_df_joint, flipped_df], ignore_index=True)\n",
    "flipped_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38563</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_41_56_808.jpg</td>\n",
       "      <td>0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38564</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_42_13_486.jpg</td>\n",
       "      <td>0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38565</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_34_05_436.jpg</td>\n",
       "      <td>-0.069032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38566</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_45_57_896.jpg</td>\n",
       "      <td>0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38567</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_45_24_146.jpg</td>\n",
       "      <td>0.059708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image     value\n",
       "38563  IMG/flipped/right_2016_12_01_13_41_56_808.jpg  0.059708\n",
       "38564  IMG/flipped/right_2016_12_01_13_42_13_486.jpg  0.059708\n",
       "38565  IMG/flipped/right_2016_12_01_13_34_05_436.jpg -0.069032\n",
       "38566  IMG/flipped/right_2016_12_01_13_45_57_896.jpg  0.059708\n",
       "38567  IMG/flipped/right_2016_12_01_13_45_24_146.jpg  0.059708"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models on joint and flipped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_flipped = generator_df(flipped_df, data_columns=['image'], val_column='value', batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16_flipped = get_model('VGG16')\n",
    "model_vgg16_flipped.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nvidia_flipped = get_model('NVIDIA')\n",
    "model_nvidia_flipped.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'vgg16_flipped', 'weights_vgg16_flipped.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver_vgg = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'nvidia_flipped', 'weights_nvidia_flipped.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver_nvidia = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4821/4821 [==============================] - 101s 21ms/step - loss: 0.0274 - val_loss: 0.0105\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.01-0.0105.hdf5\n",
      "Epoch 2/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.02-0.0097.hdf5\n",
      "Epoch 3/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.03-0.0088.hdf5\n",
      "Epoch 4/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0084 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.04-0.0086.hdf5\n",
      "Epoch 5/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0080 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.05-0.0086.hdf5\n",
      "Epoch 6/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0077 - val_loss: 0.0085\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.06-0.0085.hdf5\n",
      "Epoch 7/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0076 - val_loss: 0.0085\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.07-0.0085.hdf5\n",
      "Epoch 8/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0073 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.08-0.0086.hdf5\n",
      "Epoch 9/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0071 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.09-0.0086.hdf5\n",
      "Epoch 10/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0070 - val_loss: 0.0088\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.10-0.0088.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_vgg16_flipped = model_vgg16_flipped.fit_generator(train_generator_flipped, steps_per_epoch=len(flipped_df)/BATCH_SIZE,\n",
    "                                     validation_data = val_generator, validation_steps=len(test_df)/BATCH_SIZE,\n",
    "                                 callbacks=[saver_vgg], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4821/4821 [==============================] - 75s 15ms/step - loss: 0.0926 - val_loss: 0.0127\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.01-0.0127.hdf5\n",
      "Epoch 2/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.0121 - val_loss: 0.0139\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.02-0.0139.hdf5\n",
      "Epoch 3/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.0129 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.03-0.0166.hdf5\n",
      "Epoch 4/10\n",
      "4821/4821 [==============================] - 74s 15ms/step - loss: 1401.7355 - val_loss: 1.6685\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.04-1.6685.hdf5\n",
      "Epoch 5/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 1.3187 - val_loss: 0.3458\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.05-0.3458.hdf5\n",
      "Epoch 6/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.1029 - val_loss: 0.0382\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.06-0.0382.hdf5\n",
      "Epoch 7/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.0252 - val_loss: 0.0299\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.07-0.0299.hdf5\n",
      "Epoch 8/10\n",
      "4821/4821 [==============================] - 74s 15ms/step - loss: 4535.9729 - val_loss: 0.2767\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.08-0.2767.hdf5\n",
      "Epoch 9/10\n",
      "4821/4821 [==============================] - 74s 15ms/step - loss: 0.1495 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.09-0.0921.hdf5\n",
      "Epoch 10/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.0577 - val_loss: 0.0541\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.10-0.0541.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_nvidia_flipped = model_nvidia_flipped.fit_generator(train_generator_flipped, steps_per_epoch=len(flipped_df)/BATCH_SIZE,\n",
    "                                     validation_data = val_generator, validation_steps=len(test_df)/BATCH_SIZE,\n",
    "                                 callbacks=[saver_nvidia], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try add another dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg16_e import get_VGG16_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16e = get_VGG16_e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16e.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'vgg16e_flipped', 'weights_vgg16e_flipped.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver_vgg = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4821/4821 [==============================] - 107s 22ms/step - loss: 0.2014 - val_loss: 0.0108\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.01-0.0108.hdf5\n",
      "Epoch 2/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0119 - val_loss: 0.0116\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.02-0.0116.hdf5\n",
      "Epoch 3/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0113 - val_loss: 0.0113\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.03-0.0113.hdf5\n",
      "Epoch 4/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0094 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.04-0.0097.hdf5\n",
      "Epoch 5/10\n",
      "4821/4821 [==============================] - 105s 22ms/step - loss: 0.0088 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.05-0.0089.hdf5\n",
      "Epoch 6/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0084 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.06-0.0086.hdf5\n",
      "Epoch 7/10\n",
      "4821/4821 [==============================] - 105s 22ms/step - loss: 0.0082 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.07-0.0086.hdf5\n",
      "Epoch 8/10\n",
      "4821/4821 [==============================] - 105s 22ms/step - loss: 0.0080 - val_loss: 0.0087\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.08-0.0087.hdf5\n",
      "Epoch 9/10\n",
      "4821/4821 [==============================] - 105s 22ms/step - loss: 0.0079 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.09-0.0086.hdf5\n",
      "Epoch 10/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0078 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.10-0.0086.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_vgg16e_flipped = model_vgg16e.fit_generator(train_generator_flipped, steps_per_epoch=len(flipped_df)/BATCH_SIZE,\n",
    "                                     validation_data = val_generator, validation_steps=len(test_df)/BATCH_SIZE,\n",
    "                                 callbacks=[saver_vgg], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_vgg16.json'), 'w') as f:\n",
    "    json.dump(history_vgg16.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_nvidia.json'), 'w') as f:\n",
    "    json.dump(history_nvidia.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_vgg16_joint.json'), 'w') as f:\n",
    "    json.dump(history_vgg16_joint.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_vgg16_flipped.json'), 'w') as f:\n",
    "    json.dump(history_vgg16_flipped.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_vgg16e_flipped.json'), 'w') as f:\n",
    "    json.dump(history_vgg16e_flipped.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_nvidia_joint.json'), 'w') as f:\n",
    "    json.dump(history_nvidia_joint.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_nvidia_flipped.json'), 'w') as f:\n",
    "    json.dump(history_nvidia_flipped.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('datasets', 'train_df.p'), 'wb') as f:\n",
    "    pickle.dump(train_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('datasets', 'train_df_joint.p'), 'wb') as f:\n",
    "    pickle.dump(train_df_joint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('datasets', 'train_df_flipped.p'), 'wb') as f:\n",
    "    pickle.dump(flipped_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('datasets', 'test_df.p'), 'wb') as f:\n",
    "    pickle.dump(test_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
