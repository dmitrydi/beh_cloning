{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import keras as K\n",
    "import tensorflow as tf\n",
    "import os, json\n",
    "from scipy import misc\n",
    "import cv2\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnn_builder import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'data/simulator_data/jungle_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data = pd.read_csv(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33212 entries, 0 to 33211\n",
      "Data columns (total 7 columns):\n",
      "center      33212 non-null object\n",
      "left        33212 non-null object\n",
      "right       33212 non-null object\n",
      "steering    33212 non-null float64\n",
      "throttle    33212 non-null float64\n",
      "brake       33212 non-null float64\n",
      "speed       33212 non-null float64\n",
      "dtypes: float64(4), object(3)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "image_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>steering</th>\n",
       "      <th>throttle</th>\n",
       "      <th>brake</th>\n",
       "      <th>speed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ce...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/le...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ri...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.05696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ce...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/le...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ri...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.51839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ce...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/le...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ri...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.95285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ce...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/le...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ri...</td>\n",
       "      <td>0.012876</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.28686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ce...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/le...</td>\n",
       "      <td>data/simulator_data/jungle/run-8-jungle/IMG/ri...</td>\n",
       "      <td>0.218884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.69115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              center  \\\n",
       "0  data/simulator_data/jungle/run-8-jungle/IMG/ce...   \n",
       "1  data/simulator_data/jungle/run-8-jungle/IMG/ce...   \n",
       "2  data/simulator_data/jungle/run-8-jungle/IMG/ce...   \n",
       "3  data/simulator_data/jungle/run-8-jungle/IMG/ce...   \n",
       "4  data/simulator_data/jungle/run-8-jungle/IMG/ce...   \n",
       "\n",
       "                                                left  \\\n",
       "0  data/simulator_data/jungle/run-8-jungle/IMG/le...   \n",
       "1  data/simulator_data/jungle/run-8-jungle/IMG/le...   \n",
       "2  data/simulator_data/jungle/run-8-jungle/IMG/le...   \n",
       "3  data/simulator_data/jungle/run-8-jungle/IMG/le...   \n",
       "4  data/simulator_data/jungle/run-8-jungle/IMG/le...   \n",
       "\n",
       "                                               right  steering  throttle  \\\n",
       "0  data/simulator_data/jungle/run-8-jungle/IMG/ri...  0.000000       1.0   \n",
       "1  data/simulator_data/jungle/run-8-jungle/IMG/ri...  0.000000       1.0   \n",
       "2  data/simulator_data/jungle/run-8-jungle/IMG/ri...  0.000000       1.0   \n",
       "3  data/simulator_data/jungle/run-8-jungle/IMG/ri...  0.012876       1.0   \n",
       "4  data/simulator_data/jungle/run-8-jungle/IMG/ri...  0.218884       1.0   \n",
       "\n",
       "   brake     speed  \n",
       "0    0.0  21.05696  \n",
       "1    0.0  21.51839  \n",
       "2    0.0  21.95285  \n",
       "3    0.0  22.28686  \n",
       "4    0.0  22.69115  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_path = 'data/simulator_data/jungle/flipped'\n",
    "filenames = []\n",
    "angles = []\n",
    "for i, row in image_data.iterrows():\n",
    "    imgname = row['center']\n",
    "    imgname = imgname.strip()\n",
    "    steering_angle = row['steering']\n",
    "    img = cv2.imread(imgname)\n",
    "    img = cv2.flip(img, flipCode=1)\n",
    "    saving_name = os.path.join(flipped_path, os.path.split(imgname)[-1])\n",
    "    cv2.imwrite(saving_name, img)\n",
    "    filenames.append(saving_name)\n",
    "    angles.append(-1*steering_angle)\n",
    "    \n",
    "flipped_df = pd.DataFrame({'center': filenames, 'steering': angles})\n",
    "flipped_df.to_csv(os.path.join(flipped_path, 'driving_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_df = pd.concat([image_data, flipped_df], ignore_index =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "ind_train, ind_test = train_test_split(np.array(range(len(joint_df))), train_size=0.8, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = joint_df.loc[ind_train]\n",
    "test_df = joint_df.loc[ind_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brake</th>\n",
       "      <th>center</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>speed</th>\n",
       "      <th>steering</th>\n",
       "      <th>throttle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54817</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data/simulator_data/jungle/flipped/center_2018...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.304721</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32067</th>\n",
       "      <td>0.0</td>\n",
       "      <td>data/simulator_data/jungle/run-7-jungle/IMG/ce...</td>\n",
       "      <td>data/simulator_data/jungle/run-7-jungle/IMG/le...</td>\n",
       "      <td>data/simulator_data/jungle/run-7-jungle/IMG/ri...</td>\n",
       "      <td>17.74552</td>\n",
       "      <td>-0.296137</td>\n",
       "      <td>0.452646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42345</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data/simulator_data/jungle/flipped/center_2018...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.463519</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60873</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data/simulator_data/jungle/flipped/center_2018...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.356223</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51368</th>\n",
       "      <td>NaN</td>\n",
       "      <td>data/simulator_data/jungle/flipped/center_2018...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.291845</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       brake                                             center  \\\n",
       "54817    NaN  data/simulator_data/jungle/flipped/center_2018...   \n",
       "32067    0.0  data/simulator_data/jungle/run-7-jungle/IMG/ce...   \n",
       "42345    NaN  data/simulator_data/jungle/flipped/center_2018...   \n",
       "60873    NaN  data/simulator_data/jungle/flipped/center_2018...   \n",
       "51368    NaN  data/simulator_data/jungle/flipped/center_2018...   \n",
       "\n",
       "                                                    left  \\\n",
       "54817                                                NaN   \n",
       "32067  data/simulator_data/jungle/run-7-jungle/IMG/le...   \n",
       "42345                                                NaN   \n",
       "60873                                                NaN   \n",
       "51368                                                NaN   \n",
       "\n",
       "                                                   right     speed  steering  \\\n",
       "54817                                                NaN       NaN -0.304721   \n",
       "32067  data/simulator_data/jungle/run-7-jungle/IMG/ri...  17.74552 -0.296137   \n",
       "42345                                                NaN       NaN  0.463519   \n",
       "60873                                                NaN       NaN -0.356223   \n",
       "51368                                                NaN       NaN  0.291845   \n",
       "\n",
       "       throttle  \n",
       "54817       NaN  \n",
       "32067  0.452646  \n",
       "42345       NaN  \n",
       "60873       NaN  \n",
       "51368       NaN  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_df(samples_df_, source_path='.', data_columns = ['center'], val_column = 'steering', batch_size=4):\n",
    "# yields batches from dataframe samples_df: ['images', 'steering']\n",
    "    samples_df = samples_df_.copy()\n",
    "    num_samples = len(samples_df)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        shuffle(samples_df)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples_df[offset:offset+batch_size]\n",
    "\n",
    "            images = None\n",
    "            angles = np.array([], dtype='float32')\n",
    "            for i, batch_sample in batch_samples.iterrows():\n",
    "                name = batch_sample[np.random.choice(data_columns, 1)].values[0]\n",
    "                name = name.strip()\n",
    "                center_image = cv2.imread(os.path.join(source_path,name))\n",
    "                if center_image is not None:\n",
    "                    center_angle = batch_sample[val_column]\n",
    "                    if images is None:\n",
    "                        images = center_image[np.newaxis]\n",
    "                    else:\n",
    "                        images = np.vstack([images, center_image[np.newaxis]])\n",
    "                    angles = np.append(angles, center_angle)\n",
    "\n",
    "            yield images, angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_generator_df(samples_df_, source_path='data', data_columns = ['center'], batch_size=4):\n",
    "# yields batches for predictions, no shuffling\n",
    "    samples_df = samples_df_.copy()\n",
    "    num_samples = len(samples_df)\n",
    "    while 1: # Loop forever so the generator never terminates\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples_df[offset:offset+batch_size]\n",
    "\n",
    "            images = None\n",
    "            angles = np.array([], dtype='float32')\n",
    "            for i, batch_sample in batch_samples.iterrows():\n",
    "                name = batch_sample[np.random.choice(data_columns, 1)].values[0]\n",
    "                name = name.strip()\n",
    "                center_image = cv2.imread(os.path.join(source_path,name))\n",
    "                center_angle = batch_sample['steering']\n",
    "                if images is None:\n",
    "                    images = center_image[np.newaxis]\n",
    "                else:\n",
    "                    images = np.vstack([images, center_image[np.newaxis]])\n",
    "                angles = np.append(angles, center_angle)\n",
    "\n",
    "            yield images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "train_generator = generator_df(train_df, batch_size=BATCH_SIZE)\n",
    "val_generator = generator_df(test_df, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training VGG16-based model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVING_PATH = 'trained_weights/jungle_weights/vgg16'\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'weights_vgg16.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16 = get_model('VGG16_e')\n",
    "model_vgg16.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6643/6642 [==============================] - 151s 23ms/step - loss: 0.2040 - val_loss: 0.0286\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.01-0.0286.hdf5\n",
      "Epoch 2/10\n",
      "6643/6642 [==============================] - 151s 23ms/step - loss: 0.0283 - val_loss: 0.0245\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.02-0.0245.hdf5\n",
      "Epoch 3/10\n",
      "6643/6642 [==============================] - 150s 23ms/step - loss: 0.0236 - val_loss: 0.0221\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.03-0.0221.hdf5\n",
      "Epoch 4/10\n",
      "6643/6642 [==============================] - 151s 23ms/step - loss: 0.0217 - val_loss: 0.0212\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.04-0.0212.hdf5\n",
      "Epoch 5/10\n",
      "6643/6642 [==============================] - 151s 23ms/step - loss: 0.0205 - val_loss: 0.0210\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.05-0.0210.hdf5\n",
      "Epoch 6/10\n",
      "6643/6642 [==============================] - 150s 23ms/step - loss: 0.0197 - val_loss: 0.0206\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.06-0.0206.hdf5\n",
      "Epoch 7/10\n",
      "6643/6642 [==============================] - 150s 23ms/step - loss: 0.0190 - val_loss: 0.0207\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.07-0.0207.hdf5\n",
      "Epoch 8/10\n",
      "6643/6642 [==============================] - 150s 23ms/step - loss: 0.0185 - val_loss: 0.0205\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.08-0.0205.hdf5\n",
      "Epoch 9/10\n",
      "6643/6642 [==============================] - 150s 23ms/step - loss: 0.0181 - val_loss: 0.0202\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.09-0.0202.hdf5\n",
      "Epoch 10/10\n",
      "6643/6642 [==============================] - 150s 23ms/step - loss: 0.0177 - val_loss: 0.0207\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/jungle_weights/vgg16/weights_vgg16.10-0.0207.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_vgg16 = model_vgg16.fit_generator(train_generator, steps_per_epoch=len(train_df)/BATCH_SIZE, validation_data=val_generator,\n",
    "                                       validation_steps=len(test_df)/BATCH_SIZE, epochs=10, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training NVIDIA-style model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia import get_NVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SAVING_PATH = 'trained_weights/jungle_weights/nvidia'\n",
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'weights_nvidia.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nvidia = get_NVIDIA()\n",
    "model_nvidia.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6643/6642 [==============================] - 120s 18ms/step - loss: 0.0572 - val_loss: 0.0415\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.01-0.0415.hdf5\n",
      "Epoch 2/10\n",
      "6643/6642 [==============================] - 119s 18ms/step - loss: 0.0432 - val_loss: 0.0357\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.02-0.0357.hdf5\n",
      "Epoch 3/10\n",
      "6643/6642 [==============================] - 117s 18ms/step - loss: 0.0386 - val_loss: 0.0316\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.03-0.0316.hdf5\n",
      "Epoch 4/10\n",
      "6643/6642 [==============================] - 116s 17ms/step - loss: 0.0356 - val_loss: 0.0290\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.04-0.0290.hdf5\n",
      "Epoch 5/10\n",
      "6643/6642 [==============================] - 116s 17ms/step - loss: 0.0334 - val_loss: 0.0272\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.05-0.0272.hdf5\n",
      "Epoch 6/10\n",
      "6643/6642 [==============================] - 116s 17ms/step - loss: 0.0318 - val_loss: 0.0297\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.06-0.0297.hdf5\n",
      "Epoch 7/10\n",
      "6643/6642 [==============================] - 116s 17ms/step - loss: 0.0307 - val_loss: 0.0329\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.07-0.0329.hdf5\n",
      "Epoch 8/10\n",
      "6643/6642 [==============================] - 116s 17ms/step - loss: 0.0296 - val_loss: 0.0296\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.08-0.0296.hdf5\n",
      "Epoch 9/10\n",
      "6643/6642 [==============================] - 116s 17ms/step - loss: 0.0296 - val_loss: 0.0286\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.09-0.0286.hdf5\n",
      "Epoch 10/10\n",
      "6643/6642 [==============================] - 116s 17ms/step - loss: 0.0287 - val_loss: 0.0321\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/jungle_weights/nvidia/weights_nvidia.10-0.0321.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_nvidia = model_nvidia.fit_generator(train_generator, steps_per_epoch=len(train_df)/BATCH_SIZE, validation_data=val_generator,\n",
    "                                       validation_steps=len(test_df)/BATCH_SIZE, epochs=10, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_vgg16.history['val_loss'])\n",
    "plt.plot(history_nvidia.history['val_loss'])\n",
    "#plt.ylim((0,0.025))\n",
    "plt.legend(['vgg16', 'nvidia'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pretrained model (for experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16.load_weights('trained_weights/vgg16/weights_vgg16.05-0.0095.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to use left and right images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_generator = predict_generator_df(train_df, source_path='data', data_columns = ['center'], batch_size=BATCH_SIZE)\n",
    "left_generator = predict_generator_df(train_df, data_columns=['left'], batch_size=BATCH_SIZE)\n",
    "right_generator = predict_generator_df(train_df, data_columns=['right'], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_predictions = model_vgg16.predict_generator(center_generator, steps=len(train_df)/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_predictions = center_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_predictions = model_vgg16.predict_generator(left_generator, steps=len(train_df)/BATCH_SIZE).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "right_predictions = model_vgg16.predict_generator(right_generator, steps=len(train_df)/BATCH_SIZE).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we calculate correction factors between predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_delta = np.mean(center_predictions - left_predictions)\n",
    "right_delta = np.mean(center_predictions - right_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05206264 0.059707746\n"
     ]
    }
   ],
   "source": [
    "print(left_delta, right_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make extended dataset with all images used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_steering = train_df['steering'].values - left_delta\n",
    "right_steering = train_df['steering'].values - right_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>center</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>steering</th>\n",
       "      <th>throttle</th>\n",
       "      <th>brake</th>\n",
       "      <th>speed</th>\n",
       "      <th>left_steering</th>\n",
       "      <th>right_steering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3422</th>\n",
       "      <td>IMG/center_2016_12_01_13_38_25_085.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_38_25_085.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_38_25_085.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.186300</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>-0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6214</th>\n",
       "      <td>IMG/center_2016_12_01_13_43_37_055.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_43_37_055.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_43_37_055.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.186570</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>-0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3501</th>\n",
       "      <td>IMG/center_2016_12_01_13_38_33_072.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_38_33_072.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_38_33_072.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.823975</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>-0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2688</th>\n",
       "      <td>IMG/center_2016_12_01_13_37_10_714.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_37_10_714.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_37_10_714.jpg</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.186650</td>\n",
       "      <td>0.052063</td>\n",
       "      <td>-0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5023</th>\n",
       "      <td>IMG/center_2016_12_01_13_41_36_264.jpg</td>\n",
       "      <td>IMG/left_2016_12_01_13_41_36_264.jpg</td>\n",
       "      <td>IMG/right_2016_12_01_13_41_36_264.jpg</td>\n",
       "      <td>-0.135712</td>\n",
       "      <td>0.985533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.185320</td>\n",
       "      <td>-0.083649</td>\n",
       "      <td>-0.195420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      center  \\\n",
       "3422  IMG/center_2016_12_01_13_38_25_085.jpg   \n",
       "6214  IMG/center_2016_12_01_13_43_37_055.jpg   \n",
       "3501  IMG/center_2016_12_01_13_38_33_072.jpg   \n",
       "2688  IMG/center_2016_12_01_13_37_10_714.jpg   \n",
       "5023  IMG/center_2016_12_01_13_41_36_264.jpg   \n",
       "\n",
       "                                       left  \\\n",
       "3422   IMG/left_2016_12_01_13_38_25_085.jpg   \n",
       "6214   IMG/left_2016_12_01_13_43_37_055.jpg   \n",
       "3501   IMG/left_2016_12_01_13_38_33_072.jpg   \n",
       "2688   IMG/left_2016_12_01_13_37_10_714.jpg   \n",
       "5023   IMG/left_2016_12_01_13_41_36_264.jpg   \n",
       "\n",
       "                                       right  steering  throttle  brake  \\\n",
       "3422   IMG/right_2016_12_01_13_38_25_085.jpg  0.000000  0.985533    0.0   \n",
       "6214   IMG/right_2016_12_01_13_43_37_055.jpg  0.000000  0.985533    0.0   \n",
       "3501   IMG/right_2016_12_01_13_38_33_072.jpg  0.000000  0.000000    0.0   \n",
       "2688   IMG/right_2016_12_01_13_37_10_714.jpg  0.000000  0.985533    0.0   \n",
       "5023   IMG/right_2016_12_01_13_41_36_264.jpg -0.135712  0.985533    0.0   \n",
       "\n",
       "          speed  left_steering  right_steering  \n",
       "3422  30.186300       0.052063       -0.059708  \n",
       "6214  30.186570       0.052063       -0.059708  \n",
       "3501   9.823975       0.052063       -0.059708  \n",
       "2688  30.186650       0.052063       -0.059708  \n",
       "5023  30.185320      -0.083649       -0.195420  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_ext = train_df\n",
    "train_df_ext['left_steering'] = train_df['steering'] - left_delta\n",
    "train_df_ext['right_steering'] = train_df['steering'] - right_delta\n",
    "train_df_ext.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19284 entries, 0 to 19283\n",
      "Data columns (total 2 columns):\n",
      "image    19284 non-null object\n",
      "value    19284 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 301.4+ KB\n"
     ]
    }
   ],
   "source": [
    "train_df_joint = train_df_ext[['center', 'steering']].rename(columns={'center': 'image', 'steering': 'value'})\n",
    "train_df_joint = pd.concat([train_df_joint,\n",
    "                           train_df_ext[['left', 'left_steering']].rename(columns={'left': 'image', 'left_steering': 'value'})],\n",
    "                          ignore_index=True)\n",
    "train_df_joint = pd.concat([train_df_joint,\n",
    "                           train_df_ext[['right', 'right_steering']].rename(columns={'right': 'image', 'right_steering': 'value'})],\n",
    "                          ignore_index=True)\n",
    "train_df_joint.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VGG16 model on joint dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_joint = generator_df(train_df_joint, data_columns = ['image'], val_column = 'value', batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'vgg16_joint', 'weights_vgg16_joint.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16_joint = get_model('VGG16')\n",
    "model_vgg16_joint.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0665 - val_loss: 0.0099\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.01-0.0099.hdf5\n",
      "Epoch 2/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0095 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.02-0.0095.hdf5\n",
      "Epoch 3/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0085 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.03-0.0089.hdf5\n",
      "Epoch 4/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0085 - val_loss: 0.0093\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.04-0.0093.hdf5\n",
      "Epoch 5/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0082 - val_loss: 0.0090\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.05-0.0090.hdf5\n",
      "Epoch 6/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0078 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.06-0.0089.hdf5\n",
      "Epoch 7/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0075 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.07-0.0089.hdf5\n",
      "Epoch 8/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0072 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.08-0.0095.hdf5\n",
      "Epoch 9/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0071 - val_loss: 0.0094\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.09-0.0094.hdf5\n",
      "Epoch 10/10\n",
      "2411/2410 [==============================] - 46s 19ms/step - loss: 0.0068 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/vgg16_joint/weights_vgg16_joint.10-0.0095.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_vgg16_joint = model_vgg16_joint.fit_generator(train_generator_joint, steps_per_epoch=len(train_df_joint)/BATCH_SIZE,\n",
    "                                                        validation_data=val_generator,\n",
    "                                       validation_steps=len(test_df)/BATCH_SIZE, epochs=10, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NVIDIA-model on joint dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'nvidia_joint', 'weights_nvidia_joint.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nvidia_joint = get_model('NVIDIA')\n",
    "model_nvidia_joint.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2411/2410 [==============================] - 37s 15ms/step - loss: 0.3003 - val_loss: 0.0110\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.01-0.0110.hdf5\n",
      "Epoch 2/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0094 - val_loss: 0.0108\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.02-0.0108.hdf5\n",
      "Epoch 3/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0090 - val_loss: 0.0145\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.03-0.0145.hdf5\n",
      "Epoch 4/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0086 - val_loss: 0.0194\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.04-0.0194.hdf5\n",
      "Epoch 5/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0102 - val_loss: 0.0123\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.05-0.0123.hdf5\n",
      "Epoch 6/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0102 - val_loss: 0.0136\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.06-0.0136.hdf5\n",
      "Epoch 7/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 1148.6888 - val_loss: 0.5850\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.07-0.5850.hdf5\n",
      "Epoch 8/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.2296 - val_loss: 0.0756\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.08-0.0756.hdf5\n",
      "Epoch 9/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0617 - val_loss: 0.0469\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.09-0.0469.hdf5\n",
      "Epoch 10/10\n",
      "2411/2410 [==============================] - 36s 15ms/step - loss: 0.0639 - val_loss: 0.0447\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/nvidia_joint/weights_nvidia_joint.10-0.0447.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_nvidia_joint = model_nvidia_joint.fit_generator(train_generator_joint, steps_per_epoch=len(train_df_joint)/BATCH_SIZE,\n",
    "                                                        validation_data=val_generator,\n",
    "                                       validation_steps=len(test_df)/BATCH_SIZE, epochs=10, callbacks=[saver])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_img_path = os.path.join( 'IMG', 'flipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = []\n",
    "values = []\n",
    "for i, row in train_df_joint.iterrows():\n",
    "    imgname = row['image']\n",
    "    steering = row['value']\n",
    "    imgname = imgname.strip()\n",
    "    #img = cv2.imread(os.path.join('data',imgname))\n",
    "    #img = cv2.flip(img, flipCode=1)\n",
    "    saving_name = os.path.join(flipped_img_path, os.path.split(imgname)[-1])\n",
    "    #cv2.imwrite(saving_name, img)\n",
    "    filenames.append(saving_name)\n",
    "    values.append(-1*steering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_df = pd.DataFrame({'image': filenames, 'value': values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 38568 entries, 0 to 38567\n",
      "Data columns (total 2 columns):\n",
      "image    38568 non-null object\n",
      "value    38568 non-null float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 602.7+ KB\n"
     ]
    }
   ],
   "source": [
    "flipped_df = pd.concat([train_df_joint, flipped_df], ignore_index=True)\n",
    "flipped_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38563</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_41_56_808.jpg</td>\n",
       "      <td>0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38564</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_42_13_486.jpg</td>\n",
       "      <td>0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38565</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_34_05_436.jpg</td>\n",
       "      <td>-0.069032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38566</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_45_57_896.jpg</td>\n",
       "      <td>0.059708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38567</th>\n",
       "      <td>IMG/flipped/right_2016_12_01_13_45_24_146.jpg</td>\n",
       "      <td>0.059708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image     value\n",
       "38563  IMG/flipped/right_2016_12_01_13_41_56_808.jpg  0.059708\n",
       "38564  IMG/flipped/right_2016_12_01_13_42_13_486.jpg  0.059708\n",
       "38565  IMG/flipped/right_2016_12_01_13_34_05_436.jpg -0.069032\n",
       "38566  IMG/flipped/right_2016_12_01_13_45_57_896.jpg  0.059708\n",
       "38567  IMG/flipped/right_2016_12_01_13_45_24_146.jpg  0.059708"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flipped_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models on joint and flipped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator_flipped = generator_df(flipped_df, data_columns=['image'], val_column='value', batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16_flipped = get_model('VGG16')\n",
    "model_vgg16_flipped.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nvidia_flipped = get_model('NVIDIA')\n",
    "model_nvidia_flipped.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'vgg16_flipped', 'weights_vgg16_flipped.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver_vgg = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'nvidia_flipped', 'weights_nvidia_flipped.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver_nvidia = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4821/4821 [==============================] - 101s 21ms/step - loss: 0.0274 - val_loss: 0.0105\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.01-0.0105.hdf5\n",
      "Epoch 2/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0093 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.02-0.0097.hdf5\n",
      "Epoch 3/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0089 - val_loss: 0.0088\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.03-0.0088.hdf5\n",
      "Epoch 4/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0084 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.04-0.0086.hdf5\n",
      "Epoch 5/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0080 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.05-0.0086.hdf5\n",
      "Epoch 6/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0077 - val_loss: 0.0085\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.06-0.0085.hdf5\n",
      "Epoch 7/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0076 - val_loss: 0.0085\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.07-0.0085.hdf5\n",
      "Epoch 8/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0073 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.08-0.0086.hdf5\n",
      "Epoch 9/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0071 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.09-0.0086.hdf5\n",
      "Epoch 10/10\n",
      "4821/4821 [==============================] - 100s 21ms/step - loss: 0.0070 - val_loss: 0.0088\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/vgg16_flipped/weights_vgg16_flipped.10-0.0088.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_vgg16_flipped = model_vgg16_flipped.fit_generator(train_generator_flipped, steps_per_epoch=len(flipped_df)/BATCH_SIZE,\n",
    "                                     validation_data = val_generator, validation_steps=len(test_df)/BATCH_SIZE,\n",
    "                                 callbacks=[saver_vgg], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4821/4821 [==============================] - 75s 15ms/step - loss: 0.0926 - val_loss: 0.0127\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.01-0.0127.hdf5\n",
      "Epoch 2/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.0121 - val_loss: 0.0139\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.02-0.0139.hdf5\n",
      "Epoch 3/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.0129 - val_loss: 0.0166\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.03-0.0166.hdf5\n",
      "Epoch 4/10\n",
      "4821/4821 [==============================] - 74s 15ms/step - loss: 1401.7355 - val_loss: 1.6685\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.04-1.6685.hdf5\n",
      "Epoch 5/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 1.3187 - val_loss: 0.3458\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.05-0.3458.hdf5\n",
      "Epoch 6/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.1029 - val_loss: 0.0382\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.06-0.0382.hdf5\n",
      "Epoch 7/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.0252 - val_loss: 0.0299\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.07-0.0299.hdf5\n",
      "Epoch 8/10\n",
      "4821/4821 [==============================] - 74s 15ms/step - loss: 4535.9729 - val_loss: 0.2767\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.08-0.2767.hdf5\n",
      "Epoch 9/10\n",
      "4821/4821 [==============================] - 74s 15ms/step - loss: 0.1495 - val_loss: 0.0921\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.09-0.0921.hdf5\n",
      "Epoch 10/10\n",
      "4821/4821 [==============================] - 73s 15ms/step - loss: 0.0577 - val_loss: 0.0541\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/nvidia_flipped/weights_nvidia_flipped.10-0.0541.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_nvidia_flipped = model_nvidia_flipped.fit_generator(train_generator_flipped, steps_per_epoch=len(flipped_df)/BATCH_SIZE,\n",
    "                                     validation_data = val_generator, validation_steps=len(test_df)/BATCH_SIZE,\n",
    "                                 callbacks=[saver_nvidia], epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try add another dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vgg16_e import get_VGG16_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16e = get_VGG16_e()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vgg16e.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = os.path.join(MODEL_SAVING_PATH, 'vgg16e_flipped', 'weights_vgg16e_flipped.{epoch:02d}-{val_loss:.4f}.hdf5')\n",
    "saver_vgg = ModelCheckpoint(saving_path, verbose=1, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4821/4821 [==============================] - 107s 22ms/step - loss: 0.2014 - val_loss: 0.0108\n",
      "\n",
      "Epoch 00001: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.01-0.0108.hdf5\n",
      "Epoch 2/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0119 - val_loss: 0.0116\n",
      "\n",
      "Epoch 00002: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.02-0.0116.hdf5\n",
      "Epoch 3/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0113 - val_loss: 0.0113\n",
      "\n",
      "Epoch 00003: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.03-0.0113.hdf5\n",
      "Epoch 4/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0094 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00004: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.04-0.0097.hdf5\n",
      "Epoch 5/10\n",
      "4821/4821 [==============================] - 105s 22ms/step - loss: 0.0088 - val_loss: 0.0089\n",
      "\n",
      "Epoch 00005: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.05-0.0089.hdf5\n",
      "Epoch 6/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0084 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00006: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.06-0.0086.hdf5\n",
      "Epoch 7/10\n",
      "4821/4821 [==============================] - 105s 22ms/step - loss: 0.0082 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00007: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.07-0.0086.hdf5\n",
      "Epoch 8/10\n",
      "4821/4821 [==============================] - 105s 22ms/step - loss: 0.0080 - val_loss: 0.0087\n",
      "\n",
      "Epoch 00008: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.08-0.0087.hdf5\n",
      "Epoch 9/10\n",
      "4821/4821 [==============================] - 105s 22ms/step - loss: 0.0079 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00009: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.09-0.0086.hdf5\n",
      "Epoch 10/10\n",
      "4821/4821 [==============================] - 106s 22ms/step - loss: 0.0078 - val_loss: 0.0086\n",
      "\n",
      "Epoch 00010: saving model to trained_weights/vgg16e_flipped/weights_vgg16e_flipped.10-0.0086.hdf5\n"
     ]
    }
   ],
   "source": [
    "history_vgg16e_flipped = model_vgg16e.fit_generator(train_generator_flipped, steps_per_epoch=len(flipped_df)/BATCH_SIZE,\n",
    "                                     validation_data = val_generator, validation_steps=len(test_df)/BATCH_SIZE,\n",
    "                                 callbacks=[saver_vgg], epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_vgg16.json'), 'w') as f:\n",
    "    json.dump(history_vgg16.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_nvidia.json'), 'w') as f:\n",
    "    json.dump(history_nvidia.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_vgg16_joint.json'), 'w') as f:\n",
    "    json.dump(history_vgg16_joint.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_vgg16_flipped.json'), 'w') as f:\n",
    "    json.dump(history_vgg16_flipped.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_vgg16e_flipped.json'), 'w') as f:\n",
    "    json.dump(history_vgg16e_flipped.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_nvidia_joint.json'), 'w') as f:\n",
    "    json.dump(history_nvidia_joint.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('train_history', 'history_nvidia_flipped.json'), 'w') as f:\n",
    "    json.dump(history_nvidia_flipped.history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('datasets', 'train_df.p'), 'wb') as f:\n",
    "    pickle.dump(train_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('datasets', 'train_df_joint.p'), 'wb') as f:\n",
    "    pickle.dump(train_df_joint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('datasets', 'train_df_flipped.p'), 'wb') as f:\n",
    "    pickle.dump(flipped_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('datasets', 'test_df.p'), 'wb') as f:\n",
    "    pickle.dump(test_df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
